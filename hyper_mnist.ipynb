{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist-hyper.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": []
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "zXhDvrQMLW0y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Example of hyperparameter selection on MNIST\n",
        "\n",
        "This is a simple/fast example of hyperparameter tuning using an ensemble of DCNNs on MNIST. \n",
        "\n",
        "## Approaches\n",
        "\n",
        "Currently, some of most interesting approaches for automatic hyperparameter tuning are __[HORD](https://github.com/ilija139/HORD)__, __[reverse and forward HG](https://github.com/lucfra/FAR-HO)__, __[GP-EI](https://arxiv.org/abs/1206.2944)__ and __[GP-PES](https://arxiv.org/abs/1406.2541)__, these last two based on Gaussian processes.\n",
        "\n",
        "### Implementations\n",
        "\n",
        "__[Hyper-Engine](https://github.com/maxim5/hyper-engine)__ is a generic toolbox. At the moment, it just implements Bayesian Optimization. __[NeuPy](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html)__ provides a generic toolbox. It implements Bayesian Optimization, Gaussian Process with Expected Improvement, and Tree-structured Parzen Estimators. __[Bayesian Optimization](https://github.com/fmfn/BayesianOptimization)__ is a simpler library implementing Bayesian Optimization.\n",
        "\n",
        "Other related libraries are __[DeepReplay](https://github.com/dvgodoy/deepreplay)__ for visualization, __[Hyperas](https://github.com/maxpumperla/hyperas)__, a wrapper of hyperot (that searches using random search or TPE), __[Kopt](https://github.com/Avsecz/kopt)__ and __[Talos](https://github.com/autonomio/talos)__.\n",
        " \n",
        "### Optimization Algorithm\n",
        "\n",
        "Additionally, we can avoid parametizing the optimization algorithm by using __[Learning to Learn](https://github.com/deepmind/learning-to-learn)__, an automatic learning algorithm that uses LSTMs.\n",
        "\n",
        "## NN Architechture\n",
        "\n",
        "There are several possibilities to choose, including MCDNN, an ensemble of 35 DCNNs that gets one of the best accuracies with MNIST. Due to time and computing power constraints, especially given the extra time requirements of automatic hyperparameter optimization, we have chosen a much simpler architechture.\n",
        "\n",
        "The __[architechture](https://github.com/kkweon/mnist-competition)__ chosen for the NN is an ensemble of three DCNN: two simplified versions of VGG and a 50-layer ResNet. While training, it performs data augmentation by rotation, shearing and scaling.\n",
        "\n",
        "## Bayesian Optimization\n",
        "\n",
        "Due to time constraints during this short exercise, we have not been able to integrate the architechture with the implementation of HORD and thus have opted for the well-known Bayesian Optimization approach. Our work has consisted on adding Bayesian Optimization to this architechture. \n",
        "\n",
        "### Hyper-parameters \n",
        "\n",
        "Many automatic hyperparameter tuning methods, including BO, are not very good with categorical variables. In our case, most of the variables are categorical, with the exception of a few like the learning rate. The learning rate in particular is a variable that should be tested on a logarithmic scale. For that reason, we have turned it into a categorical value, opting for one of a list of possible values separated by factors of 10.\n",
        "\n",
        "#### Network architechture\n",
        " \n",
        "Some parameters affecting network architechture are:\n",
        "\n",
        "- Non-linear activation function used.\n",
        "- Number of dense hidden layers.\n",
        "- Neurons per hidden layer.\n",
        "- Number of convolutional layers (CNN).\n",
        "- Number of features in each layer (for a Convolutional NN).\n",
        "\n",
        "In our case, the architechture is an ensemble of three networks: ideally, we should replicate this parameters for each one of them. One possible option is to calculate the hyperparameters of each component network, and then ensemble them, but this might not lead to the best result, and is quite time consuming.\n",
        "\n",
        "Again due to time and GPU computing-power  restrictions, we have chosen to use less parameters and apply them to the three networks. In particular, we just chose to parametrize the non-linear activation function used.\n",
        "\n",
        "#### Training\n",
        "\n",
        "Among the parameters affecting the training are:\n",
        "\n",
        "- Epochs.\n",
        "- Batch-size.\n",
        "- Optimization algorithm.\n",
        "- Learning rate.\n",
        "- Decay of the learning rate.\n",
        "- Weight inizialization.\n",
        "- Data augmentation: on/off, techniques used.\n",
        "\n",
        "Due to time and GPU-computing restrictions, we decided to use these hyperparameters to the training of the each network component. These restrictions also made us limit the number of epochs to 20, limiting thus the maximum achievable accuracy. In our example, we decided to parametrize:\n",
        "\n",
        "- Epochs: 1, 10 and 20. 1 is just for search-example purposes.\n",
        "- Batch-size: 32 and 64.\n",
        "- Optimization algorithms: Adagrad, Adam and Adamax (RMSProp seemed to perform clearly worse than these).\n",
        "- Learning rate: logarithmic scale from 0.01 to 0.001 (due to time limits we did not explore more).\n",
        "- Decay of the learning rate: 0.99, 0.9, 0.85, 0.8.\n",
        "\n",
        "\n",
        "### Experiment\n",
        "\n",
        "You can repeat the experiment and achieve better results, depending on your time and GPU.\n",
        "\n",
        "#### Prerequisites\n",
        "\n",
        "If you want to repeat the experiment, you need the following environment:\n",
        "\n",
        "- Anaconda for Python 2.7\n",
        "- Sonnet: \n",
        "```bash\n",
        "conda install -c hcc dm-sonnet\n",
        "```\n",
        "- Bayesian Optimization: \n",
        "```bash\n",
        "conda install -c yikelu bayesian-optimization\n",
        "```\n",
        "\n",
        "Or, in Google Colab:\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "UtVktLqkLaBB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 149
        },
        "outputId": "00ae857b-5646-4955-c8e7-82d2e68c50e5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528409465573,
          "user_tz": -120,
          "elapsed": 4382,
          "user": {
            "displayName": "Carlos Javier",
            "photoUrl": "//lh4.googleusercontent.com/-ts-Ym2ycgtE/AAAAAAAAAAI/AAAAAAAAADM/GyXSXPYDMyg/s50-c-k-no/photo.jpg",
            "userId": "100961447717034189583"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# This is neccesary in case you did not followed the steps above or you are using Google Colab\n",
        "!pip install sonnet\n",
        "!pip install bayesian-optimization"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sonnet in /usr/local/lib/python2.7/dist-packages (0.1.6)\r\n",
            "Requirement already satisfied: networkx==1.8.1 in /usr/local/lib/python2.7/dist-packages (from sonnet) (1.8.1)\n",
            "Requirement already satisfied: bayesian-optimization in /usr/local/lib/python2.7/dist-packages (0.6.0)\n",
            "Requirement already satisfied: scipy>=0.14.0 in /usr/local/lib/python2.7/dist-packages (from bayesian-optimization) (0.19.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.0 in /usr/local/lib/python2.7/dist-packages (from bayesian-optimization) (0.19.1)\n",
            "Requirement already satisfied: numpy>=1.9.0 in /usr/local/lib/python2.7/dist-packages (from bayesian-optimization) (1.14.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TmSva5dxQb03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "#### Commands\n",
        "\n",
        "Clone the repository:"
      ]
    },
    {
      "metadata": {
        "id": "OoXOAqWCQdl2",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "outputId": "3c3cdf8b-7573-4ae4-9faa-5c74a03ff76b",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1528409502241,
          "user_tz": -120,
          "elapsed": 4832,
          "user": {
            "displayName": "Carlos Javier",
            "photoUrl": "//lh4.googleusercontent.com/-ts-Ym2ycgtE/AAAAAAAAAAI/AAAAAAAAADM/GyXSXPYDMyg/s50-c-k-no/photo.jpg",
            "userId": "100961447717034189583"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!cd /\n",
        "!git clone https://github.com/carloshavier/hyperparams-bo"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hyperparams-bo'...\n",
            "remote: Counting objects: 24, done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 24 (delta 3), reused 24 (delta 3), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (24/24), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "miM3mxM2RPUo",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir('hyperparams-bo')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2HAOlqkKXIEc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You can set the default hyperparameters as well as the valid ranges for each one of the hyperparameters by editing hypers.py.\n",
        "\n",
        "In order to search using Bayesian Optimization, we define how many initial experiments we want to run, and also the number of experiments during the search phase in hysearchbo.py.\n",
        "\n",
        "In particular, in:\n",
        "\n",
        "```python\n",
        "bo.maximize(init_points=3, n_iter=5, kappa=2)\n",
        "```\n",
        "\n",
        "To explore the search space trying to maximize the accuracy and finally visualise the results, you just need to run:\n",
        "\n",
        "```bash\n",
        "!python hysearchbo.py\n",
        "```"
      ]
    },
    {
      "metadata": {
        "id": "HjgHUo0zXRg0",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Warning: depending on the range of search of the hyperparameters and the number of items to explore (bo.maximize), the following command might take several hours/days.\n",
        "!python hysearchbo.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3iGzrw2oXjrQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Results\n",
        "\n",
        "#### Bayesian Optimization\n",
        "\n",
        "The following graph shows the result of our experiment. In particular, it shows the combinations of values explored by Bayesian Optimization and their reached accuracy, by color (green equals max accuracy). Note that due to time and computing power restrictions, during this experiment we limited the number of epochs to just 20.\n",
        "\n",
        "![BO search of best hyperparameters](https://raw.githubusercontent.com/carloshavier/hyperparams-bo/master/experiment-results/experiment.png)\n",
        "\n",
        "The maximum accuracy was obtained using:\n",
        "\n",
        "- Non-linear activation function: PReLU.\n",
        "- Epochs: 10.\n",
        "- Batch-size: 32.\n",
        "- Optimization algorithm: Adam.\n",
        "- Learning rate: 0.01.\n",
        "- Decay of the learning rate: 0.99.\n",
        "\n",
        "The ensemble accuracy was $93.55\\%$. The raw data is __[here](https://raw.githubusercontent.com/carloshavier/hyperparams-bo/master/experiment-results/ex-7-6-18-01-small.txt)__.\n",
        "\n",
        "#### Manual Optimization\n",
        "\n",
        "After looking at the results obtained by BO, we did a few more experiments, including logarithmic changes of the learning rate, learning decay and optimizer. We finally push the epochs to 50, as we were doing just a few experiments. Our best result was obtained with:\n",
        "\n",
        "- Non-linear activation function: PReLU\n",
        "- Epochs: 50\n",
        "- Batch-size: 64\n",
        "- Optimization algorithm: Adam\n",
        "- Learning rate: 0.01\n",
        "- Decay of the learning rate: 0.95\n",
        "\n",
        "The obtained accuracy was:\n",
        "\n",
        "- Model-0: $97.76\\%\n",
        "$\n",
        "- Model-1: $94.87\\%\n",
        "$\n",
        "- Model-2: $97.27\\%\n",
        "$\n",
        "- Final Test Accuracy: **$98.30\\%$**"
      ]
    }
  ]
}